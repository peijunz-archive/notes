{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"mathmacros\" style=\"display:none;\">\n",
    "$\\def\\bra#1{\\mathinner{\\langle{#1}|}}\n",
    "\\def\\ket#1{\\mathinner{|{#1}\\rangle}}\n",
    "\\def\\braket#1{\\mathinner{\\langle{#1}\\rangle}}\n",
    "\\newcommand{\\mdef}{\\overset{\\mathrm{def}}{=}}\n",
    "\\newcommand{\\bm}{\\mathbf}\n",
    "\\newcommand{\\inv}[1]{#1^{-1}}   % Inverse Matrix\n",
    "\\newcommand{\\invt}[1]{#1^{-T}}  % Inverse Transposed Matrix\n",
    "\\renewcommand{\\nl}{\\\\&\\phantom{{}={}}}% Newline In aligned equations\n",
    "\\newcommand{\\pfr}[2]{\\frac{\\pp #1}{\\pp #2}}      % Partial derivative\n",
    "\\newcommand{\\dfr}[2]{\\frac{\\dd #1}{\\dd #2}}      % Total derivative\n",
    "\\newcommand{\\pp}{\\partial}\n",
    "\\DeclareMathOperator{\\Var}{Var}\n",
    "\\DeclareMathOperator{\\det}{det}\n",
    "\\DeclareMathOperator{\\tr}{tr}\n",
    "\\DeclareMathOperator{\\sgn}{sgn}\n",
    "\\DeclareMathOperator{\\adj}{adj}\n",
    "\\DeclareMathOperator{\\ii}{i}\n",
    "\\DeclareMathOperator{\\dd}{d}\n",
    "\\DeclareMathOperator{\\rhs}{RHS}\n",
    "\\DeclareMathOperator{\\lhs}{LHS}\n",
    "\\newcommand{\\nl}{\\\\&\\phantom{={}}}\n",
    "\\DeclareMathOperator{\\E}{E}\n",
    "\\DeclareMathOperator{\\Cov}{Cov}\n",
    "\\DeclareMathOperator{\\Beta}{B}\n",
    "\\DeclareMathOperator{\\Bdist}{Beta}$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feb 21\n",
    "+ How diff norm of one site density matrix Scale with respect to depth and length\n",
    "+ Maximum norm of difference between $\\rho$\n",
    "+ $E=0$ way\n",
    "+ For large width of g, look at var of $x$ component of sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feb 14\n",
    "+ Obviously $\\rho$ is not diagonalized with local optimization. Here are [examples](http://nbviewer.jupyter.org/github/peijunz/notes/blob/master/rho.ipynb) \n",
    "+ Optimization of Variance $\\langle H^2\\rangle - \\langle H\\rangle^2$ is very different from optimizing expection value $\\langle (H-E)^2\\rangle$\n",
    "    + For variance there are multiple local minimums, while there are only one minimum for expection value.\n",
    "    + Finding a local minimum for variance is hard, but it is very easy for expectation value.\n",
    "\n",
    "\n",
    "#### Test $V$ related code\n",
    "After contraction, we will have local $V_{ijkl}$ and we want to optimize $V_{ijkl}U_{ij}U^\\dagger_{kl}$. This optimization code related to $V$ is fully tested. \n",
    "+ Tested derivatives by numerical differentiation for both variance $V(H^2)_{ijkl}U_{ij}U^\\dagger_{kl}-[V(H)_{ijkl}U_{ij}U^\\dagger_{kl}]^2$ and expectation value for $V(h)_{ijkl}U_{ij}U^\\dagger_{kl}$\n",
    "+ Tested optimization is able to get global minimum for expectation value\n",
    "+ Tested optimization is able to get local minimum for Variance.\n",
    "\n",
    "#### Test contraction code\n",
    "As exact solution for min variance is hard, I was mainly testing minimization of expectation value for any Hermitian operator. For local Hamiltonian like $H=\\sum_i \\sigma_z^i$, $H^2$ is not local, which makes optimization harder.\n",
    "\n",
    "Local Operator can be optimized by local transformations exactly. Below are some test cases:\n",
    "+ Minimize expection value of $h=\\sum h_i$(sum of local observable terms), and $h=\\prod_i h_i$\n",
    "+ Minimize variance for product of local terms $h=\\bigotimes h_i$\n",
    "\n",
    "\n",
    "There are also reverse engineering tests for layered tensor network. It needs an optimal $\\rho$ as a starting point\n",
    "+ If we are optimizing Variance $\\langle H^2\\rangle - \\langle H\\rangle^2$, it is __almost__ impossible.\n",
    "+ If we are optimizing $\\langle (H-E)^2\\rangle$, it is viable. So we only test this case.\n",
    "\n",
    "As we are doing __local optimization__ for nonlocal observable, there may be another minimum closer to our initial condition. Suppose $U$ is random unitary matrix, then $U(A)=\\exp(A\\log U), 0<A<1$ is a weaker transformation than $U$. Only for small deviation applied(e.g. $A\\sim 0.1$), it works pretty well to recover the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feb 7\n",
    "+ [1408.0535](https://arxiv.org/abs/1408.0535) Ising model Equation 7\n",
    "+ What does $\\rho$ looks like in local optimization\n",
    "+ Test for \"Local\" Hamiltonian\n",
    "+ Reverse engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jan 31\n",
    "+ Many-body localization in a disordered quantum Ising chain. [1403.1568](https://arxiv.org/pdf/1403.1568.pdf)\n",
    "+ Locally take $\\langle\\sigma_z\\rangle$, subsystem $\\rho$ and compare to Gibbs ensemble.\n",
    "+ **Done.** Initial state? Divide $S$ evenly in the beginning. Vary initial $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jan 24\n",
    "+ Split chain into three parts and use different initial $\\rho$, to change eigenvalues. The depth should be at least prop to length of chain.\n",
    "+ **Done.** Do the optimization code for $f=A_{\\alpha\\beta\\gamma\\delta}V_{\\alpha\\beta}V^+_{\\gamma\\delta}$\n",
    "+ **Done.** Do contraction of $f$ in the exact way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anushya's Suggestions\n",
    "Why is this useful for distinguishing MBL and thermalizing phases: fix $E$ and the starting total entropy of the initial density matrices rho_0. For example, we could pick $\\rho_0 = \\prod_i (\\alpha_i \\ket{\\uparrow} + \\sqrt{1-\\alpha_i^2} \\ket{\\downarrow}$ ) with $\\alpha_i$ close to 0 or 1 on each site. In the ETH phase, the optimized density matrices must reproduce the same expectation value for local observables. In the MBL phase, we would hope to see a broader distribution of values for local observables determined by the z-product state with maximum weight in $\\rho_0$. The other thing we could do is monitor the depth of the circuit required for variance minimization as a function of the starting entropy. We expect that the smaller the starting entropy density, the larger the depth of the circuit. How should the depth scale with N for fixed entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "### Low level\n",
    "+ Do ED, explicity a diagonal matrix with various $N$ and classical $S$, turn them into MPOs, and check bond dimension.\n",
    "+ Does the optimized $\\rho$ always mean steady state?\n",
    "    + For $\\rho(\\delta)=e^{i\\delta}\\rho_0 e^{-i\\delta}$\n",
    "$$\\nabla_\\delta\\Var[H]=i[\\rho_0, H^2-2\\langle H\\rangle H], \\quad \\langle H\\rangle=\\mathrm{tr}[H\\rho_0]$$\n",
    "\n",
    "### High level\n",
    "+ Cleaner way to tell difference between ETH/MBL\n",
    "+ Entanglement threshold (for density matrices) using finite depth circuit to generate a steady state.\n",
    "    + What does entanglement threshold here mean?\n",
    "    + For MPO, try two layers local optimization on about 8 sites first.\n",
    "\n",
    "### Others\n",
    "+ Try to minimize $\\tr[(H-E_0)^2\\rho]$?\n",
    "    + The optimization code for this routine is handy\n",
    "    + Zero energy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
